{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## job1\n",
    "\n",
    "Entry Level Sales Consultant\n",
    "\n",
    "[link]https://www.indeed.com/q-Entry-Level-Consulting-l-Miami,-FL-jobs.html?advn=5881671747126226&vjk=2a92760904dc0a71"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## job2\n",
    "\n",
    "Big Data Consultant\n",
    "\n",
    "[link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt        \n",
    "from collections import Counter        \n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "  \n",
    "book = xlwt.Workbook() # create a new excel file\n",
    "sheet_test = book.add_sheet('word_count') # add a new sheet\n",
    "i = 0\n",
    "sheet_test.write(i,0,'word') # write the header of the first column\n",
    "sheet_test.write(i,1,'count') # write the header of the second column\n",
    "sheet_test.write(i,2,'ratio') # write the header of the third column\n",
    "    \n",
    "with open('job_one.txt','r',encoding='utf-8', errors = 'ignore') as text_word: # define the location of your txt file\n",
    "     \n",
    "    # convert all the word into lower cases\n",
    "    # filter out stop words\n",
    "    word_list = [i for i in text_word.read().lower().split() if i not in stop]\n",
    "    word_total = word_list.__len__()\n",
    "     \n",
    "    count_result =  Counter(word_list)\n",
    "    for result in count_result.most_common(10):\n",
    "        i = i+1 \n",
    "        sheet_test.write(i,0,result[0])\n",
    "        sheet_test.write(i,1,result[1])\n",
    "        sheet_test.write(i,2,(result[1]/word_total))\n",
    "    \n",
    "book.save('job_one.xls')# define the location of your excel file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"job_one.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt        \n",
    "from collections import Counter        \n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "  \n",
    "book = xlwt.Workbook() # create a new excel file\n",
    "sheet_test = book.add_sheet('word_count') # add a new sheet\n",
    "i = 0\n",
    "sheet_test.write(i,0,'word') # write the header of the first column\n",
    "sheet_test.write(i,1,'count') # write the header of the second column\n",
    "sheet_test.write(i,2,'ratio') # write the header of the third column\n",
    "    \n",
    "with open('job_two.txt','r',encoding='utf-8', errors = 'ignore') as text_word: # define the location of your txt file\n",
    "     \n",
    "    # convert all the word into lower cases\n",
    "    # filter out stop words\n",
    "    word_list = [i for i in text_word.read().lower().split() if i not in stop]\n",
    "    word_total = word_list.__len__()\n",
    "     \n",
    "    count_result =  Counter(word_list)\n",
    "    for result in count_result.most_common(10):\n",
    "        i = i+1 \n",
    "        sheet_test.write(i,0,result[0])\n",
    "        sheet_test.write(i,1,result[1])\n",
    "        sheet_test.write(i,2,(result[1]/word_total))\n",
    "    \n",
    "book.save('job_two.xls')# define the location of your excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"job_two.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'includes', 'development,', 'would', 'employer', 'perfect', 'Description', 'gender,', 'product', 'win.', 'parking', 'sales', 'customers', 'request', 'beer', 'national', 'growth;', 'screening,', 'Gainsight,', 'Technology', 'trusted', 'adopting', 'engaging', 'free', 'will:', 'product,', 'require', 'Entry', 'Other', 'Indeed', 'high', 'company', 'taking', 'sessions', 'any', 'account', 'serve', 'Player.', 'countries.', 'sex,', 'relationship', 'opportunity', 'onboarding', 'training', 'Must', 'Take', 'are.', 'Individual', 'satisfaction.', 'presence', 'Zendesk', 'Beer', 'characteristic', 'industries', 'uncapped', 'employment', 'providers', 'Eager', 'building', 'expanding', 'government,', 'regard', 'weekly,', 'retention', 'worldwide', 'drink', '(portion', 'ensuring', 'corporate', 'Development', 'success', 'monthly,', 'Level', 'venture', 'plus', 'educate,', 'Health', 'FL', 'Own', 'Sales,', 'approach', 'Participate', 'escalations', '#1', 'let', 'commission.', 'Establish', 'while', 'directly', 'goals', 'program,', 'Technical', 'religion,', 'status,', 'motivated,', 'over', 'Be', 'media,', 'with.', 'serve,', 'access', 'mental', 'offers', 'assigned', 'No', 'them!', 'alternative', 'renewals.', 'successful', 'familiarity', 'industries,', 'passionate', 'Supporting', 'just', 'member', 'Casual', 'ancestry,', 'veteran', 'gym', 'well', 'cases.', 'finance,', 'backed', 'than', 'win,', 'achieve', 'origin,', 'applicable', 'paid', 'part', 'provides', 'after', 'long-term', 'please', 'customers:', 'IT', 'office', 'revenue/metrics', 'Exposure', 'Basic', 'good', 'program', 'Paid', 'without', 'years’', 'impeccable', 'marital', 'expand', 'SaaS', 'push', '20', 'healthcare,', 'more.', 'supporting,', 'disability,', 'You’re', '(yes,', 'lot', 'management.', 'wide', 'process.', 'so', 'You', 'equal', 'companies', 'hitting', 'law.', 'orientation,', 'retail,', 'http://www.kaseya.com.', 'all', 'priority.', 'Location:', 'applicants', 'win/win', 'space.', 'company)', 'cross-sells,', 'adapt', 'Kaseya', 'which', 'more,', 'consultancy', 'use.', 'targets.', 'mid-market', 'around.', 'base', \"employer's\", 'Sales', 'role', 'salary', 'training,', 'learn', 'sexual', '10,000', 'must', 'race,', 'other', 'methods', 'If', 'not', 'there', 'put', 'industry/Software', 'Miami,', 'care', 'entering', 'learn,', 'service', '2-week', 'able', 'consistent', 'training.', 'competitive', 'Full-time', 'resulting', 'desire', 'visit', 'working!)', 'ongoing', 'technology,', '0-4', 'exceeding', '401K', 'Fridays', 'protected', 'grow.', 'attire', 'advisor', 'Company', 'attitude', 'They', 'application', 'day', '(Brickell)', 'parties', 'you,', 'offer,', 'thereby', 'excel;', 'manufacturing,', 'Salesforce,', 'quarterly', 'management', 'Flexible', 'adoption,', 'love', 'book', 'seek', 'coaching', 'age,', 'coachable', 'vacation', 'software', 'driving', 'upsells,', 'headquarters', 'work;', 'responsible', 'success,', 'it', 'it’s', 'around', 'Position'}\n",
      "{'designing,', 'architecture', 'area', 'linking', 'technologies', 'process', 'From', 'modernizing', 'multi-Terabyte', 'design,', 'include', 'Consultants', 'standards,', 'continuously', 'up', 'inform', 'HBase,', 'metadata,', 'make', 'meet', 'MapReduce,', 'offer', 'Work', 'hidden', 'fast-changing', '(NoSQL,', 'continuous', 'job', '(GCP)', 'monetization', 'practice,', 'media', 'range', 'ingestion,', 'unstructured', 'maintain', 'robotics,', 'needs.', 'information', 'day.', 'weblog,', 'Web', 'logical', 'techniques', 'storage', 'recruiting', 'integration', '(HDFS),', 'Function', 'Azure,', 'prototyping,', 'led', 'focus', 'relational', 'Certification,', 'always', '—', 'warehousing,', 'enable', 'grow', '1+', 'Certification', 'days', 'functions', 'small', 'culture', 'purpose', '80-100%', 'them', 'lives.', 'exploring', 'Data', 'Cognitive', 'File', 'professionals', 'decision', 'skill-based', 'management,', 'portfolio', 'flexibility', 'cleansing', 'ecosystems', 'encourages', 'voice,', 'deliver', 'visualization', 'reporting', 'enterprise', 'research:', 'As-a-Service', 'changing', 'weekly', 'know', 'scenarios', 'Amazon', 'decision-making.', 'industry', 'what', 'batch', 'plan', 'developing', 'win', 'design', 'transform', 'Center.', 'implementation.', 'structure,', 'leveraging', 'visualization,', '(MS', 'advanced', 'believe', 'formal', 'Reduce', 'improvements', 'computing', 'ways', 'scalable,', 'relationships', 'related', 'potential', 'learn.', 'on-the-job', 'HDFS', 'Center', 'hands-on', 'team.', 'lake', 'AWS', 'ETL', 'environment.', 'Integration', 'year', 'provisioning', 'embracing', 'YARN,', 'Microsoft', 'utilizing', 'teams', \"you'll\", 'Leadership', 'fact', 'presentation', 'EC2,', 'future', 'where', 'data.', 'global,', 'relevant', 'need', 'choices', 'helping', 'to:', 'to.', 'large-scale', 'Distributed', 'feel', 'maintaining', 'hands', 'Google', 'models.', 'development', 'dynamic', 'System', 'master', 'Explore', 'enterprise-scale', 'Flume,', 'uniqueness', 'BigInsights,', 'within', 'level', 'transform.', 'prepared', 'cognitive', 'Deloitte', 'Information', 'analytics', 'Recruiter', 'important', 'integrators', 'career.', 'typical', 'Intelligence', 'warehousing', 'Sqoop,', 'strengths', 'acquisition', 'Drive', 'positive', 'generate', 'etc.', '(BDE),', 'enhanced', 'client-based', 'you’ll', 'Map', 'requirements,', 'applications', 'supporting', 'excel', 'troves', 'Spark,', 'processes', 'chosen', 'impact', 'custom', 'Key', 'Edition', 'manage', 'non-relational', 'mean', 'giving,', 'engagement', 'leadership', 'At', 'processes.', 'strategies', 'there’s', 'PowerPoint).', 'confident.', 'Delivery', 'dramatically', 'How', 'organizational', 'leaders,', 'standardization,', 'Visio,', 'workstreams', 'preferred.', 'differentiated', 'purpose:', 'interview,', 'leverages', 'Strategy', 'transformation,', 'celebrate', 'In', 'individuals', 'Preferred:', 'happy', 'Consultant,', 'University,', 'Our', 'data,', 'vast', 'age', 'supportive', 'cloud', 'modernization', 'disruption,', 'Defining', 'matters.', 'Experience', '#IND:PTY', 'testing', 'looking', 'extends', 'ingestion.', 'centered,', '(AWS),', 'becoming', 'continue', 'An', 'do', 'Power', 'skills,', 'you.', 'ecosystems,', 'cloud-based', 'sharpen', 'Required:', 'operational', 'efficiency', 'room', 'organization.', 'clients’', 'hybrid', 'IBM', 'benefits.', 'opportunities', 'tips', 'Deloitte’s', 'table', 'matching', 'MongoDB,', 'traditional', 'responsibilities', 'well-being', 'rationalization,', 'Cassandra)', 'Big', 'insights,', 'real', 'dimension', 'Kafka,', 'least', 'modeling,', 'ingestion', 'broad', 'Learn', 'way', 'new', 'aware.', 'markets.', 'sensor,', 'applying', 'Knowledge', 'like', 'specialization', 'Services', 'written', 'defines', 'structured', 'predict', 'Cloud', 'quality', 'clients,', 'clusters.', 'insights', 'Willingness', 'sets', 'tables,', 'Elastic', 'technologies;', 'Deloitte.', 'Hadoop', 'Thursday/Friday)', 'Platform', 'system', '(Monday', 'Strong', 'value', 'enabling', 'equivalent', 'points', 'organizations', 'offering', 'products', 'focuses', 'navigate', 'routines', 'experiences', 'implementation,', 'data-driven', 'you’re', 'years', 'core', 'healthy,', 'database', 'drive', 'distributed', 'sourcing', 'Degree', 'Corporate', 'social', 'architecting', 'programs', 'delivery,', 'Implement', 'offerings', 'machine', 'Check', 'automation,', 'throughout', 'analytics,', 'We', 'communities.', 'into', 'capabilities.', 'strategy,', 'time', '30+', 'Analytics', '–', 'entry-level', '2+', 'learning', 'professionals.', 'some', 'suggest', 'trends', 'The', 'video,', 'metadata', 'making', 'processes,', 'Life', 'identify', 'Warehouses', 'actions', 'prescribe', 'addition', 'most', 'data', 'science-based', 'leading', 'consulting', 'Bachelor’s', 'recognizing', 'senior', 'oral', 'between', 'lead', 'intelligence', 'designing', 'confident,', 'consumption', 'confidence,', 'involving', 'travel,', 'implementing', 'Pig,', 'Azure', 'Hive,', 'uncover', 'supply', 'people', 'Storm', 'platforms', 'Spark', 'volunteerism,', 'background', 'organization', 'expertise', 'MS', 'chain', 'science', 'enhance', 'implementations,', 'Informatica', 'solutions,', 'Together', 'Understanding', 'using', 'governance', 'Talend', 'seekers', 'inspire', 'helps', 'clear,', 'power', 'communication', 'architecture,', 'Deloitte,', 'Leverage'}\n"
     ]
    }
   ],
   "source": [
    "with open ('job_one.txt','r',encoding='utf-8',errors = 'ignore')as job_one: #this part we haven't covered so you prob dont understand yet\n",
    "    with open('job_two.txt','r',encoding='utf-8',errors = 'ignore')as job_two:\n",
    "        jobone_str = job_one.read()\n",
    "        jobtwo_str = job_two.read()\n",
    "       \n",
    "              \n",
    "        jobone_set = set(jobone_str.split())  \n",
    "        jobtwo_set = set(jobtwo_str.split())\n",
    "        \n",
    "        print(jobone_set.difference(jobtwo_set)) \n",
    "        print(jobtwo_set.difference(jobone_set))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "with open ('job_one.txt','r',encoding='utf-8',errors = 'ignore')as job_one: #this part we haven't covered so you prob dont understand yet\n",
    "    with open('job_two.txt','r',encoding='utf-8',errors = 'ignore')as job_two:\n",
    "        jobone_str = job_one.read()\n",
    "        jobtwo_str = job_two.read()\n",
    "        \n",
    "        print(fuzz.token_sort_ratio(jobone_str,jobtwo_str))\n",
    "              \n",
    "    #    jobone_set = set(jobone_str.split())  \n",
    "     #   jobtwo_set = set(jobtwo_str.split())\n",
    "        \n",
    "   #    print(jobone_set.difference(jobtwo_set)) \n",
    "    #    print(jobtwo_set.difference(jobone_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
